
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{A Tiny Reproduction of CutMix on CIFAR-100 and Tiny ImageNet}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ryan T. Jordan \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Electrical and Computer Engineering\\
Purdue University\\
West Lafayette, IN 47907, USA \\
\texttt{jorda227@purdue.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We revisit the CutMix data augmentation strategy on smaller image benchmarks. The original CutMix paper reports consistent improvements over baseline, MixUp, and Cutout on ImageNet-1k and other datasets. We design a “tiny reproduction” using ResNet-18 with linear probing and fine-tuning on CIFAR-100 and Tiny ImageNet, comparing Baseline, Cutout, MixUp, and CutMix under a unified training pipeline. Contrary to the original findings, none of the augmentations improve top-1 or top-5 accuracy over the baseline in our setting. We analyze this discrepancy in terms of dataset size, training schedule, and augmentation hyperparameters, and discuss implications for applying these methods on small, noisy datasets.
\end{abstract}

\section{Introduction}

Deep convolutional neural networks (CNNs) achieve state-of-the-art performance on many vision benchmarks, including image classification, detection, and segmentation \citep{krizhevsky_imagenet_2012}. As models and datasets grow, simple data augmentation and regularization strategies---such as random crops, flips, and dropout---have become essential for improving generalization and robustness. More recent work has proposed augmentation methods that deliberately corrupt or mix training examples to act as stronger regularizers, encouraging models to rely on a broader set of features and improving robustness to occlusion and distribution shift.

Cutout~\citep{devries_improved_2017}, MixUp~\citep{zhang_mixup_2018}, and CutMix~\citep{yun_cutmix_2019} are three widely studied examples. Cutout removes a contiguous square patch from an image, forcing the classifier to rely on other object parts. MixUp linearly interpolates pairs of images and labels in pixel space, encouraging approximately linear behavior between training examples and improving calibration. CutMix combines these ideas by cutting a rectangular region from one image and pasting it into another while mixing the labels in proportion to the visible area. The original CutMix paper reports that this simple local patch replacement strategy consistently improves over strong baselines, MixUp, and Cutout across CIFAR-10/100, ImageNet-1k, weakly supervised object localization (WSOL), transfer learning, and robustness benchmarks when combined with high-capacity backbones and long training schedules \citep{yun_cutmix_2019}.

However, most of these evaluations are conducted in large-scale, carefully tuned settings \citep{devries_improved_2017,zhang_mixup_2018,yun_cutmix_2019}. In practice, many users operate under more constrained conditions: smaller datasets, limited compute budgets, and relatively simple training recipes. It is not obvious a priori that the gains reported for CutMix and related mixed-sample augmentations will transfer unchanged to such a small-scale regime, or that the same hyperparameters and training schedules remain appropriate.

In this work, we perform a \emph{tiny reproduction} of the CutMix classification results in a small-scale, single-GPU setting. Rather than attempting to replicate the full breadth of experiments from the original paper, we focus on its core claim: that CutMix outperforms a strong baseline and related augmentations such as MixUp and Cutout. We deliberately simplify the original setup while preserving its main ingredients. Specifically, we evaluate on two smaller benchmarks, CIFAR-100 and Tiny ImageNet; we use an ImageNet-pretrained ResNet-18 backbone with a two-stage training procedure (linear probe followed by full fine-tuning); and within a unified pipeline we compare a strong baseline using standard spatial transforms against three augmentation strategies: Cutout, MixUp, and CutMix.

Our study is organized as a reproduction of the original CutMix claims under this simplified regime. We closely mirror their comparison between baseline, MixUp, Cutout, and CutMix, but under a much shorter training schedule and with a smaller backbone. We then analyze where our findings align with or diverge from the original results, and discuss possible explanations in terms of dataset scale, optimization budget, and augmentation strength. In particular, we ask: \emph{Do the reported gains from CutMix and related mixed-sample augmentations persist on small, noisy datasets when training an ImageNet-pretrained ResNet-18 under realistic compute constraints?} Our experiments suggest that the answer is more nuanced than the large-scale results alone would indicate, and highlight the importance of validating augmentation strategies in the target operating regime.


\section{Methods}

\subsection{Overall design}
Our goal is to perform a ``tiny reproduction'' of the CutMix paper's core classification result---that CutMix improves over a strong baseline and related augmentations such as MixUp and Cutout---under a constrained, single-GPU setting. We construct a unified training pipeline based on an ImageNet-pretrained ResNet-18 backbone and evaluate four augmentation configurations on each dataset:
\begin{enumerate}
    \item \textbf{Baseline}: standard spatial augmentations only,
    \item \textbf{Cutout}: Baseline + Cutout,
    \item \textbf{MixUp}: Baseline + MixUp,
    \item \textbf{CutMix}: Baseline + CutMix.
\end{enumerate}
All configurations share the same architecture, optimizer, and training schedule, and differ only in the augmentation applied to each batch. Training proceeds in two stages---linear probe (LP) followed by full fine-tuning (FT)---and we reuse the LP checkpoint across all augmentation variants to reduce compute and ensure a consistent initialization. We run this pipeline on CIFAR-100 and Tiny ImageNet and report top-1 and top-5 accuracy on the validation split.

\subsection{Datasets}

\paragraph{CIFAR-100.}
CIFAR-100 consists of 50{,}000 training and 10{,}000 test images of size $32\times 32$ across 100 classes \citep{krizhevsky2009learning}. We treat the official training split as the training set and the test split as the evaluation set. Images are normalized channel-wise using dataset statistics. For the Baseline configuration, we apply a random horizontal flip with probability 0.5 and a random crop with padding around the original $32\times 32$ image. No additional color jitter or advanced augmentations are used beyond MixUp, Cutout, and CutMix when those are enabled.

\paragraph{Tiny ImageNet.}
Tiny ImageNet is a downscaled subset of ImageNet with 200 classes and 500 training and 50 validation images per class~\citep{le_tiny_2015}. Images are resized to $64\times 64$ and normalized using dataset statistics. We use the official training split for training and the validation split for evaluation. Baseline augmentations mirror those used for CIFAR-100, adapted to the larger resolution: random resized crop (or random crop with padding) and random horizontal flip, followed by normalization. No label smoothing or other regularizers are applied unless otherwise specified.

\subsection{Model and training procedure}

We use a ResNet-18 architecture with the final fully connected layer modified to match the number of classes in each dataset (100 for CIFAR-100 and 200 for Tiny ImageNet). The backbone is initialized from an ImageNet-pretrained ResNet-18 checkpoint from \emph{Deep Residual Learning for Image Recognition}~\citep{he_deep_2016}. All models are trained with stochastic gradient descent (SGD) with Nesterov momentum, using momentum $0.9$, weight decay $5 \times 10^{-4}$, and a batch size of 256. Training consists of a linear probe stage followed by fine-tuning.

\paragraph{Linear probe (LP).}
We freeze all convolutional layers and train only the final classification head for 20 epochs with a constant learning rate of $3 \times 10^{-3}$, without any learning-rate warmup or decay. This quickly adapts the pretrained backbone to the target dataset at low cost. The resulting parameters are saved as the LP checkpoint.

\paragraph{Fine-tuning (FT).}
Starting from the LP checkpoint, we unfreeze the entire network and train all parameters jointly for 30 epochs. We use the same optimizer settings as above but reduce the base learning rate to $3 \times 10^{-4}$. During FT we apply a warmup--cosine learning-rate schedule~\citep{loshchilov_sgdr_2017}: the learning rate is linearly increased from a fraction of the base value during an initial warmup period and then decayed according to a cosine schedule over the remaining epochs. Warmup is used only in FT; the LP stage uses a fixed learning rate. For each dataset, we run LP once and reuse the resulting checkpoint as the starting point for all four augmentation configurations; each configuration then performs its own 30-epoch FT from this shared initialization.

\subsection{Augmentation configurations}

In all configurations, augmentations act only on training images; validation images are left unmodified except for normalization.

\paragraph{Baseline (standard transforms).}
The Baseline pipeline applies the spatial transforms described above (random horizontal flip, random crop / random resized crop, normalization) and no mixed-sample augmentation. This serves as the reference for all comparisons.

\paragraph{Cutout.}
We implement Cutout using PyTorch's \texttt{RandomErasing} transform. A square patch covering 25\% of the image area is erased with probability 0.5 and replaced with zeros. Unlike the CutMix paper's implementation of Cutout---where the patch center may lie outside the image and the erased region can extend beyond the image boundary---\texttt{RandomErasing} constrains the patch to lie entirely within the image. Apart from this erased region, the image and label remain unchanged.

\paragraph{MixUp and CutMix.}
MixUp and CutMix follow the standard formulations in~\citep{zhang_mixup_2018,yun_cutmix_2019}: for each minibatch, we form synthetic examples by mixing pairs of images and labels using a Beta-distributed mixing coefficient applied globally (MixUp) or within a randomly sampled rectangular region (CutMix). We fix the corresponding hyperparameters ($\alpha$ for MixUp and $\beta$ for CutMix) for all experiments on a given dataset.

\subsection{Evaluation protocol}

For each dataset and augmentation configuration, we evaluate the best validation-loss checkpoint from the FT stage and report top-1 and top-5 accuracy on the CIFAR-100 test split and Tiny ImageNet validation split. Unless otherwise stated, results are from a single training run per configuration; the codebase supports multiple seeds for future variance estimates. This controlled setup---shared architecture, optimizer, schedule, and evaluation protocol---allows us to attribute performance differences primarily to the choice of augmentation.


\section{Results}

\subsection{CIFAR-100}

\subsection{Tiny ImageNet}

\subsection{Comparison to original CutMix results}

\section{Analysis and Discussion}

\subsection{Why do augmentations hurt here?}

\subsection{What this says about the original claims}

\subsection{Threats to validity}

\section{Conclusions and Future Work}

\section{References}

\section{Appendix}

\subsection{Citations within the text}


\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
You may include other additional sections here.


\end{document}
