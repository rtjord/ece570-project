{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtjord/ece570-project/blob/main/ECE_570_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cut Once, Measure Twice: A Reproduction of CutMix’s Reported Gains"
      ],
      "metadata": {
        "id": "pGH7Z0hfEAtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Cut Once, Measure Twice: A Reproduction of CutMix’s Reported Gains](#scrollTo=pGH7Z0hfEAtY)\n",
        "\n",
        ">[Runtime Setup](#scrollTo=Fl4iaIoSETEz)\n",
        "\n",
        ">>[Set Up File Directory](#scrollTo=4zb6uHYTGawp)\n",
        "\n",
        ">>[Print Device Info and Package Versions](#scrollTo=zV1SAaGiG1Xk)\n",
        "\n",
        ">[Helpers](#scrollTo=IEoTzlq7wBZn)\n",
        "\n",
        ">>[Set Seed](#scrollTo=4URjJlVfGmKh)\n",
        "\n",
        ">>[Get Device](#scrollTo=Z2yIwlSWHrRN)\n",
        "\n",
        ">[Load Data](#scrollTo=5dzCEj2jusP6)\n",
        "\n",
        ">>[Make Augmentations](#scrollTo=MFQblUZxrvtO)\n",
        "\n",
        ">>[Make Transforms](#scrollTo=wlwgTPZLIVDf)\n",
        "\n",
        ">>[CIFAR-100 Data Loaders](#scrollTo=DPX6N4KUJIux)\n",
        "\n",
        ">>[Download Tiny ImageNet](#scrollTo=-9t303LUJwBC)\n",
        "\n",
        ">>[Construct Tiny ImageNet Validation](#scrollTo=wbRB1FwlKQfD)\n",
        "\n",
        ">>[Tiny ImageNet Data Loaders](#scrollTo=WZTDt77PKdEW)\n",
        "\n",
        ">[Build Model](#scrollTo=w6WhObE6vh5B)\n",
        "\n",
        ">[Train/Evaluate Model](#scrollTo=6GdW2IhbvtSt)\n",
        "\n",
        ">>[Soft Categorical Cross-Entropy](#scrollTo=06Y2AhjzO98A)\n",
        "\n",
        ">>[Plotting Helper](#scrollTo=ub-tpV1hPVU2)\n",
        "\n",
        ">>[Train One Epoch](#scrollTo=LfO7wYCjOW4e)\n",
        "\n",
        ">>[Training Loop](#scrollTo=jRmkAvixWdmc)\n",
        "\n",
        ">>[Evaluate Model](#scrollTo=m2jQXMHEvvjc)\n",
        "\n",
        ">>[Run Experiment (Train + Test)](#scrollTo=RUA1Ycgj1hS5)\n",
        "\n",
        ">[Global Config](#scrollTo=egwwakGrE2Dk)\n",
        "\n",
        ">[ResNet-50 CIFAR-100](#scrollTo=119xYLjkFQhC)\n",
        "\n",
        ">>[Baseline](#scrollTo=053-lvam1kUF)\n",
        "\n",
        ">>[CutMix](#scrollTo=Ft9_i0T49uRm)\n",
        "\n",
        ">>[Cutout](#scrollTo=RbCKSpVytbBK)\n",
        "\n",
        ">>[Mixup](#scrollTo=CPCSziqMthZG)\n",
        "\n",
        ">[ResNet-18 CIFAR-100](#scrollTo=FZjvvYroFpBh)\n",
        "\n",
        ">>[Baseline](#scrollTo=Ddt8Gj7k8Plf)\n",
        "\n",
        ">>[CutMix](#scrollTo=rmH6Wp23Ca1Z)\n",
        "\n",
        ">>[Cutout](#scrollTo=e9Y80pGNkehE)\n",
        "\n",
        ">>[MixUp](#scrollTo=_oGl1HoIkiFq)\n",
        "\n",
        ">[ResNet-50 Tiny ImageNet](#scrollTo=hIaE-DLw4z9i)\n",
        "\n",
        ">>[Baseline](#scrollTo=sab-6UTf5BDC)\n",
        "\n",
        ">>[CutMix](#scrollTo=1WnYbOdHBgEs)\n",
        "\n",
        ">[Create Figures](#scrollTo=gH90e69ybkUF)\n",
        "\n",
        ">>[Training History](#scrollTo=Lgk24c783Q7A)\n",
        "\n",
        ">>[Data Augmentation Examples](#scrollTo=TgIT35UtnnWl)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "9bf0SHckD4Y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime Setup"
      ],
      "metadata": {
        "id": "Fl4iaIoSETEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up File Directory"
      ],
      "metadata": {
        "id": "4zb6uHYTGawp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7aJu8Ed7yOL"
      },
      "outputs": [],
      "source": [
        "import os, random, platform\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib\n",
        "\n",
        "# ---- Base paths ----\n",
        "BASE_DIR = Path(\"./\")\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "CKPT_DIR = BASE_DIR / \"checkpoints\"\n",
        "RESULTS_DIR = BASE_DIR / \"results\"\n",
        "FIG_DIR = BASE_DIR / \"figures\"\n",
        "for d in [DATA_DIR, CKPT_DIR, RESULTS_DIR, FIG_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Device Info and Package Versions"
      ],
      "metadata": {
        "id": "zV1SAaGiG1Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib\n",
        "import platform\n",
        "\n",
        "# ---- Device info ----\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    cc = f\"{props.major}.{props.minor}\"\n",
        "    print(f\"Device: CUDA — {torch.cuda.get_device_name(0)} (cc {cc})\")\n",
        "    print(f\"GPU RAM: {props.total_memory/1e9:.2f} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Device: CPU\")\n",
        "\n",
        "# ---- Versions ----\n",
        "print(f\"PyTorch:     {torch.__version__} (CUDA {torch.version.cuda})\")\n",
        "print(f\"Torchvision: {torchvision.__version__}\")\n",
        "print(f\"Matplotlib:  {matplotlib.__version__}\")\n",
        "print(f\"Python:      {platform.python_version()}\")\n",
        "\n",
        "# ---- Paths summary ----\n",
        "print(f\"DATA_DIR = {DATA_DIR}\")\n",
        "print(f\"CKPT_DIR = {CKPT_DIR}\")\n",
        "print(f\"FIG_DIR  = {FIG_DIR}\")"
      ],
      "metadata": {
        "id": "nYRA2o7bG7Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEoTzlq7wBZn"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Seed\n",
        "\n"
      ],
      "metadata": {
        "id": "4URjJlVfGmKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "AMQsvOuSGwog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Device"
      ],
      "metadata": {
        "id": "Z2yIwlSWHrRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Prefer CUDA, then Apple MPS, else CPU.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "uoTdHlRyHuDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dzCEj2jusP6"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Augmentations"
      ],
      "metadata": {
        "id": "MFQblUZxrvtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision.transforms import v2 as T2\n",
        "\n",
        "\n",
        "def make_batch_aug(num_classes: int, mode: str = \"none\", p: float = 0.5):\n",
        "    \"\"\"\n",
        "    Construct a batch-level augmentation module (MixUp, CutMix, Cutout)\n",
        "    or return None if mode is 'none'/'off'.\n",
        "    \"\"\"\n",
        "    m = (mode or \"none\").lower()\n",
        "    if m in (\"none\", \"off\"):\n",
        "        return None\n",
        "\n",
        "    if m == \"mixup\":\n",
        "        return T2.RandomApply(\n",
        "            nn.ModuleList([T2.MixUp(num_classes=num_classes, alpha=1.0)]),\n",
        "            p=p,\n",
        "        )\n",
        "\n",
        "    if m == \"cutmix\":\n",
        "        return T2.RandomApply(\n",
        "            nn.ModuleList([T2.CutMix(num_classes=num_classes, alpha=1.0)]),\n",
        "            p=p,\n",
        "        )\n",
        "\n",
        "    if m == \"cutout\":\n",
        "        # Use RandomErasing to mimic Cutout-style behavior\n",
        "        return T2.RandomErasing(\n",
        "            p=p,                 # apply with probability p\n",
        "            scale=(0.25, 0.25),  # fixed area ~25% of the image\n",
        "            ratio=(1.0, 1.0),\n",
        "            value=0.0,\n",
        "            inplace=True,\n",
        "        )\n",
        "\n",
        "    raise ValueError(\"mix mode must be one of: 'none', 'mixup', 'cutmix', 'cutout'\")"
      ],
      "metadata": {
        "id": "_k2r56Tzrtdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Transforms"
      ],
      "metadata": {
        "id": "wlwgTPZLIVDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, models\n",
        "\n",
        "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
        "CIFAR100_STD  = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "def make_cifar_transforms(train: bool = True, architecture=\"resnet18\"):\n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),\n",
        "        ])\n",
        "\n",
        "def make_imagenet_transforms(train: bool = True, architecture=\"resnet18\"):\n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.RandomResizedCrop(64),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "        ])\n",
        "    else:\n",
        "        weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
        "        return weights.transforms()"
      ],
      "metadata": {
        "id": "Y1mWsl_DIX-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-100 Data Loaders"
      ],
      "metadata": {
        "id": "DPX6N4KUJIux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
        "from torchvision import datasets\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "def build_cifar100_loaders(\n",
        "    architecture: str,\n",
        "    batch_size: int,\n",
        "    num_workers: int,\n",
        "    seed: int,\n",
        "    root: str | Path = \"./data\",\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader, List[str]]:\n",
        "\n",
        "    root = Path(root)\n",
        "    pin  = torch.cuda.is_available()\n",
        "\n",
        "    # Per-sample transforms\n",
        "    tf_train = make_cifar_transforms(train=True, architecture=architecture)   # augments + ImageNet norm\n",
        "    tf_eval  = make_cifar_transforms(train=False, architecture=architecture)  # built-in eval transform\n",
        "\n",
        "    # Two logically identical train datasets, differing only by transform\n",
        "    train_ds_aug  = datasets.CIFAR100(root=str(root), train=True,  download=True,  transform=tf_train)\n",
        "    train_ds_eval = datasets.CIFAR100(root=str(root), train=True,  download=False, transform=tf_eval)\n",
        "    test_ds       = datasets.CIFAR100(root=str(root), train=False, download=True,  transform=tf_eval)\n",
        "\n",
        "    class_names = list(train_ds_aug.classes)\n",
        "\n",
        "    # Deterministic split on indices (shared across both train datasets)\n",
        "    n_total = len(train_ds_aug)\n",
        "    n_val   = int(round(n_total * 0.1))\n",
        "    n_train = n_total - n_val\n",
        "\n",
        "    # random_split returns Subset objects; we only need their indices\n",
        "    g = torch.Generator().manual_seed(seed)  # isolate the randomness of the split\n",
        "    train_subset_idx, val_subset_idx = random_split(range(n_total), [n_train, n_val], generator=g)\n",
        "    train_idx = list(train_subset_idx.indices)\n",
        "    val_idx   = list(val_subset_idx.indices)\n",
        "\n",
        "    # Subset with appropriate transforms\n",
        "    train_subset = Subset(train_ds_aug,  train_idx)  # aug pipeline\n",
        "    val_subset   = Subset(train_ds_eval, val_idx)    # eval pipeline (SAME as test)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin\n",
        "    )\n",
        "    test_loader  = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_names"
      ],
      "metadata": {
        "id": "rpHwolP4JS5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Tiny ImageNet"
      ],
      "metadata": {
        "id": "-9t303LUJwBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "def ensure_tinyimagenet(root: str | Path = \"./data\") -> Path:\n",
        "    \"\"\"\n",
        "    Ensure Tiny ImageNet is present under root.\n",
        "    Downloads and unzips into root/tiny-imagenet-200 if missing.\n",
        "    \"\"\"\n",
        "    root = Path(root)\n",
        "    target_dir = root / \"tiny-imagenet-200\"\n",
        "    zip_path = root / \"tiny-imagenet-200.zip\"\n",
        "\n",
        "    if not target_dir.exists():\n",
        "        root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Download if zip not already there\n",
        "        if not zip_path.exists():\n",
        "            print(\"Downloading Tiny ImageNet...\")\n",
        "            subprocess.run(\n",
        "                [\"wget\", \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\", \"-O\", str(zip_path)],\n",
        "                check=True\n",
        "            )\n",
        "\n",
        "        # Unzip into root\n",
        "        print(\"Unzipping Tiny ImageNet...\")\n",
        "        print(zip_path)\n",
        "        subprocess.run([\"unzip\", str(zip_path), \"-d\", str(root)], check=True)\n",
        "\n",
        "    return target_dir"
      ],
      "metadata": {
        "id": "m4AIXnR6KD4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Tiny ImageNet Validation"
      ],
      "metadata": {
        "id": "wbRB1FwlKQfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tHT7wIx-6I7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "from typing import Optional\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# --- Tiny ImageNet helpers ---\n",
        "\n",
        "class TinyImageNetVal(Dataset):\n",
        "    \"\"\"\n",
        "    Tiny-ImageNet 'val' folder uses a CSV-style annotation file instead of class subfolders.\n",
        "    This dataset reads 'val_annotations.txt' and maps each image to the corresponding wnid.\n",
        "    Expects the standard layout:\n",
        "      root/\n",
        "        train/<wnid>/*.JPEG\n",
        "        val/images/*.JPEG\n",
        "        val/val_annotations.txt  (filename, wnid, x, y, w, h ...)\n",
        "        wnids.txt (optional)\n",
        "    \"\"\"\n",
        "    def __init__(self, root: str | Path, transform=None, class_to_idx: Optional[dict] = None):\n",
        "        self.root = Path(root)\n",
        "        self.transform = transform\n",
        "        anno_path = self.root / \"val\" / \"val_annotations.txt\"\n",
        "        images_dir = self.root / \"val\" / \"images\"\n",
        "\n",
        "        # Build class_to_idx from train if not given\n",
        "        if class_to_idx is None:\n",
        "            train_dir = self.root / \"train\"\n",
        "            wnids = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "            class_to_idx = {wnid: i for i, wnid in enumerate(wnids)}\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "        # Parse annotations\n",
        "        self.samples = []  # list of (image_path, target_idx)\n",
        "        with open(anno_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\"\\t\")\n",
        "                if len(parts) < 2:\n",
        "                    parts = line.strip().split()  # fallback on whitespace\n",
        "                fname, wnid = parts[0], parts[1]\n",
        "                if wnid not in self.class_to_idx:\n",
        "                    # If class not in mapping (shouldn't happen), skip\n",
        "                    continue\n",
        "                img_path = images_dir / fname\n",
        "                self.samples.append((img_path, self.class_to_idx[wnid]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, target = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny ImageNet Data Loaders"
      ],
      "metadata": {
        "id": "WZTDt77PKdEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from typing import Tuple, List\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "def build_tinyimagenet_loaders(\n",
        "    architecture: str,\n",
        "    batch_size: int,\n",
        "    num_workers: int,\n",
        "    seed: int,\n",
        "    root: str | Path = \"./data\",\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader, List[str]]:\n",
        "    \"\"\"\n",
        "    Returns train_loader, val_loader, test_loader, class_names for Tiny ImageNet.\n",
        "    - train: ImageFolder(root/train)\n",
        "    - val: uses TinyImageNetVal (annotation file) OR ImageFolder if already reorganized\n",
        "    - test: labels are not provided in the official release; we reuse 'val' as 'test' for metrics.\n",
        "    \"\"\"\n",
        "    root = Path(root)\n",
        "    ensure_tinyimagenet(root)\n",
        "    pin = torch.cuda.is_available()\n",
        "\n",
        "    tf_train = make_imagenet_transforms(train=True, architecture=architecture)\n",
        "    tf_eval  = make_imagenet_transforms(train=False, architecture=architecture)\n",
        "\n",
        "    train_dir = root / \"tiny-imagenet-200\" / \"train\"\n",
        "\n",
        "    # Two logically identical train datasets, differing only by transform\n",
        "    train_ds_aug  = datasets.ImageFolder(str(train_dir), transform=tf_train)\n",
        "    train_ds_eval = datasets.ImageFolder(str(train_dir), transform=tf_eval)\n",
        "    assert len(train_ds_aug.classes) == 200, f\"Expected 200 train classes, got {len(train_ds_aug.classes)}\"\n",
        "    class_names = list(train_ds_aug.classes)  # wnids\n",
        "\n",
        "    # Deterministic split on indices (shared across both train datasets)\n",
        "    n_total = len(train_ds_aug)\n",
        "    n_val   = int(round(n_total * 0.1))\n",
        "    n_train = n_total - n_val\n",
        "\n",
        "    # random_split returns Subset objects; we only need their indices\n",
        "    g = torch.Generator().manual_seed(seed)  # isolate the randomness of the split\n",
        "    train_subset_idx, val_subset_idx = random_split(range(n_total), [n_train, n_val], generator=g)\n",
        "    train_idx = list(train_subset_idx.indices)\n",
        "    val_idx   = list(val_subset_idx.indices)\n",
        "\n",
        "    # Subset with appropriate transforms\n",
        "    train_subset = Subset(train_ds_aug,  train_idx)  # aug pipeline\n",
        "    val_subset   = Subset(train_ds_eval, val_idx)    # eval pipeline (SAME as test)\n",
        "\n",
        "    # Use validation set as test set\n",
        "    test_set = TinyImageNetVal(\n",
        "        root / \"tiny-imagenet-200\",\n",
        "        transform=tf_eval,\n",
        "        class_to_idx=train_ds_aug.class_to_idx\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin,\n",
        "        persistent_workers=(num_workers > 0),\n",
        "        prefetch_factor=2 if num_workers > 0 else None,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin,\n",
        "        persistent_workers=(num_workers > 0),\n",
        "        prefetch_factor=2 if num_workers > 0 else None,\n",
        "    )\n",
        "    test_loader  = DataLoader(\n",
        "        test_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin,\n",
        "        persistent_workers=(num_workers > 0),\n",
        "        prefetch_factor=2 if num_workers > 0 else None,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_names"
      ],
      "metadata": {
        "id": "Gx1qCzO8Kg64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6WhObE6vh5B"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNr4hYilvd_M"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "def build_cifar100_model(architecture: str):\n",
        "    \"\"\"\n",
        "    CIFAR-style ResNet stem: 3x3 conv, stride 1, no maxpool.\n",
        "    Good for 32x32 inputs when training from scratch.\n",
        "    \"\"\"\n",
        "    if architecture == \"resnet18\":\n",
        "        model = torchvision.models.resnet18(weights=None)\n",
        "    elif architecture == \"resnet50\":\n",
        "        model = torchvision.models.resnet50(weights=None)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown architecture {architecture}\")\n",
        "\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "    )\n",
        "    model.maxpool = nn.Identity()\n",
        "\n",
        "    model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_tiny_imagenet_model(architecture: str):\n",
        "    \"\"\"\n",
        "    Original ResNet model with fully connected layer swapped for\n",
        "    correct number of classes\n",
        "    \"\"\"\n",
        "    if architecture == \"resnet18\":\n",
        "        model = torchvision.models.resnet18(weights=None)\n",
        "    elif architecture == \"resnet50\":\n",
        "        model = torchvision.models.resnet50(weights=None)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown architecture {architecture}\")\n",
        "\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "    )\n",
        "    # model.maxpool = nn.Identity()\n",
        "\n",
        "    model.fc = nn.Linear(model.fc.in_features, 200)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GdW2IhbvtSt"
      },
      "source": [
        "# Train/Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft Categorical Cross-Entropy"
      ],
      "metadata": {
        "id": "06Y2AhjzO98A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class SoftCE(nn.Module):\n",
        "    def forward(self, logits, targets):\n",
        "        if targets.dtype in (torch.long, torch.int64):  # hard labels\n",
        "            return nn.functional.cross_entropy(logits, targets)\n",
        "        logp = nn.functional.log_softmax(logits, dim=-1)\n",
        "        return -(targets * logp).sum(dim=-1).mean()"
      ],
      "metadata": {
        "id": "5SOsfktGPEIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Helper"
      ],
      "metadata": {
        "id": "ub-tpV1hPVU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_curves(history: dict, title: str):\n",
        "    \"\"\"Plot train loss and val top-1 from a History dict.\"\"\"\n",
        "    if not history or \"train_loss\" not in history:\n",
        "        print(\"Nothing to plot.\")\n",
        "        return\n",
        "    xs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax1.plot(xs, history[\"train_loss\"], label=\"train_loss\")\n",
        "    ax1.plot(xs, history.get(\"val_loss\", []), label=\"val_loss\")\n",
        "    ax1.set_xlabel(\"epoch\"); ax1.set_ylabel(\"loss\"); ax1.grid(True, alpha=0.3)\n",
        "    ax2 = ax1.twinx()\n",
        "    if \"val_top1\" in history:\n",
        "        ax2.plot(xs, history[\"val_top1\"], \"o--\", label=\"val_top1\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Top-1 (%)\")\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines + lines2, labels + labels2, loc=\"best\")\n",
        "    plt.title(title)\n",
        "    plt.savefig(FIG_DIR / f\"{title}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GwNml_5APWr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train One Epoch"
      ],
      "metadata": {
        "id": "LfO7wYCjOW4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    *,\n",
        "    device=None,\n",
        "    batch_aug=None,\n",
        "    amp=True,\n",
        "):\n",
        "    \"\"\"One training epoch. Returns average loss.\"\"\"\n",
        "    device = device or get_device()\n",
        "    model.train()\n",
        "    criterion = SoftCE()\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\n",
        "        device=\"cuda\",\n",
        "        enabled=(amp and device.type == \"cuda\")\n",
        "    )\n",
        "    running_loss, seen = 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "        if batch_aug is not None:\n",
        "            x, y = batch_aug(x, y)  # may return soft targets\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type=device.type, enabled=(amp and device.type in {\"cuda\", \"mps\"})):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        running_loss += loss.item() * bs\n",
        "        seen += bs\n",
        "\n",
        "    return running_loss / max(1, seen)"
      ],
      "metadata": {
        "id": "PzgnSrJ7OaKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "jRmkAvixWdmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "def cpu_state_dict(model):\n",
        "    return {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    *,\n",
        "    device,\n",
        "    epochs,\n",
        "    batch_aug=None,\n",
        "    optimizer=None,\n",
        "    scheduler=None,\n",
        "    amp=True,\n",
        "    save_path=None\n",
        "):\n",
        "    model.to(device)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_top1\": [],\n",
        "        \"val_top5\": [],\n",
        "        \"lr\": []\n",
        "    }\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    best_model_checkpoint = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            device=device,\n",
        "            batch_aug=batch_aug,\n",
        "            amp=amp\n",
        "        )\n",
        "\n",
        "        val_metrics = evaluate(model, val_loader, device=device)\n",
        "        val_loss = val_metrics[\"loss\"]\n",
        "        top1 = val_metrics[\"top1\"]\n",
        "        top5 = val_metrics[\"top5\"]\n",
        "\n",
        "        if scheduler is not None:\n",
        "          # get the learning rate before stepping the scheduler\n",
        "          lr = scheduler.get_last_lr()[0]\n",
        "          scheduler.step()\n",
        "        else:\n",
        "          lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_top1\"].append(top1)\n",
        "        history[\"val_top5\"].append(top5)\n",
        "        history[\"lr\"].append(lr)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_checkpoint = {\n",
        "                  \"epoch\": epoch,\n",
        "                  \"model_state\": cpu_state_dict(model),\n",
        "            }\n",
        "\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch:03d} \"\n",
        "            f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
        "            f\"top1={top1:.2f}%  top5={top5:.2f}%  \"\n",
        "            f\"lr={lr}\"\n",
        "        )\n",
        "\n",
        "    # Save at the end once the entire training history has finished\n",
        "    if best_model_checkpoint is not None:\n",
        "      best_model_checkpoint[\"history\"] = history\n",
        "      if save_path is not None:\n",
        "          torch.save(best_model_checkpoint, save_path)\n",
        "\n",
        "    return best_model_checkpoint\n"
      ],
      "metadata": {
        "id": "1RKjmhvlVxKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2jQXMHEvvjc"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV2S_sfgvyWy"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device=None, criterion=None, max_batches=None):\n",
        "    \"\"\"Eval with hard labels (no batch-level aug). Returns dict: loss/top1/top5.\"\"\"\n",
        "    device = device or get_device()\n",
        "    model.eval()\n",
        "    criterion = criterion or nn.CrossEntropyLoss()\n",
        "\n",
        "    total, loss_sum, top1_sum, top5_sum = 0, 0.0, 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for b, (x, y) in enumerate(loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss_sum += criterion(logits, y).item() * y.size(0)\n",
        "            total += y.size(0)\n",
        "\n",
        "            # Top-1 / Top-5\n",
        "            pred1 = logits.argmax(dim=1)\n",
        "            top1_sum += (pred1 == y).sum().item()\n",
        "            k = min(5, logits.size(1))\n",
        "            pred5 = logits.topk(k, dim=1).indices\n",
        "            top5_sum += (pred5 == y.view(-1, 1)).any(dim=1).sum().item()\n",
        "\n",
        "            # Limit number of batches used for evaluation\n",
        "            if max_batches is not None and (b + 1) >= max_batches:\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / max(1, total),\n",
        "        \"top1\": 100.0 * top1_sum / max(1, total),\n",
        "        \"top5\": 100.0 * top5_sum / max(1, total),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUA1Ycgj1hS5"
      },
      "source": [
        "## Run Experiment (Train + Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cEM-WbRRcny"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torchvision.transforms import v2 as T2\n",
        "from google.colab import files\n",
        "\n",
        "def run_experiment(name: str, cfg: dict):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      {\n",
        "        'name': ...,\n",
        "        'history': {...},\n",
        "        'test_metrics': {'loss': ..., 'top1': ..., 'top5': ...},\n",
        "      }\n",
        "    \"\"\"\n",
        "    set_seed(cfg[\"seed\"])\n",
        "    device = get_device()\n",
        "\n",
        "    # build data loaders\n",
        "    if cfg[\"dataset\"] == \"cifar100\":\n",
        "        train_loader, val_loader, test_loader, class_names = build_cifar100_loaders(\n",
        "            architecture=cfg[\"architecture\"],\n",
        "            batch_size=cfg[\"batch_size\"],\n",
        "            num_workers=cfg[\"num_workers\"],\n",
        "            seed=cfg[\"seed\"],\n",
        "        )\n",
        "        model = build_cifar100_model(\n",
        "            architecture=cfg[\"architecture\"],\n",
        "        ).to(device)\n",
        "        num_classes = 100\n",
        "    elif cfg[\"dataset\"] == \"tinyimagenet\":\n",
        "        train_loader, val_loader, test_loader, class_names = build_tinyimagenet_loaders(\n",
        "            architecture=cfg[\"architecture\"],\n",
        "            batch_size=cfg[\"batch_size\"],\n",
        "            num_workers=cfg[\"num_workers\"],\n",
        "            seed=cfg[\"seed\"],\n",
        "        )\n",
        "        model = build_tiny_imagenet_model(\n",
        "            architecture=cfg[\"architecture\"],\n",
        "        ).to(device)\n",
        "        num_classes = 200\n",
        "    else:\n",
        "        raise Exception(f\"Unknown dataset {cfg[\"dataset\"]}\")\n",
        "\n",
        "    print(\"\\n--- Dataset split ---\")\n",
        "    sizes = {\n",
        "        \"train\": len(train_loader.dataset),\n",
        "        \"val\":   len(val_loader.dataset),\n",
        "        \"test\":  len(test_loader.dataset),\n",
        "    }\n",
        "    print(sizes)\n",
        "\n",
        "    # These settings are the same whether fine tuning or training from scratch\n",
        "    batch_aug = make_batch_aug(num_classes, cfg[\"aug_mode\"], p=cfg[\"aug_prob\"])\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params,\n",
        "        lr=cfg[\"lr\"],\n",
        "        momentum=0.9,\n",
        "        nesterov=True,\n",
        "        weight_decay=cfg[\"weight_decay\"],\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer,\n",
        "        milestones=[int(0.25*cfg[\"epochs\"]),\n",
        "                    int(0.50*cfg[\"epochs\"]),\n",
        "                    int(0.75*cfg[\"epochs\"])],\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Train ---\")\n",
        "    best_model_checkpoint = train(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device=device,\n",
        "        epochs=cfg[\"epochs\"],\n",
        "        batch_aug=batch_aug,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        amp=cfg[\"amp\"],\n",
        "        save_path=CKPT_DIR / f\"{name}.pt\"\n",
        "    )\n",
        "    hist = best_model_checkpoint[\"history\"]\n",
        "    plot_curves(hist, title=name)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Test ---\")\n",
        "    # Load weights from the epoch with lowest validation loss\n",
        "    model.load_state_dict(best_model_checkpoint[\"model_state\"])\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, device=device)\n",
        "    print(\n",
        "        f\"loss={test_metrics['loss']:.4f}  \"\n",
        "        f\"top1={test_metrics['top1']:.2f}%  \"\n",
        "        f\"top5={test_metrics['top5']:.2f}%\"\n",
        "    )\n",
        "\n",
        "    results = best_model_checkpoint | {\n",
        "        \"name\": name,\n",
        "        \"test_metrics\": test_metrics,\n",
        "    }\n",
        "\n",
        "    torch.save(results, RESULTS_DIR / f\"{name}.pt\")\n",
        "    files.download(RESULTS_DIR / f\"{name}.pt\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Config\n",
        "Keep these hyperparameters the same between experiments."
      ],
      "metadata": {
        "id": "egwwakGrE2Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_CONFIG = {\n",
        "    \"seed\": 42,\n",
        "    \"batch_size\": 256,\n",
        "    \"num_workers\": 4,\n",
        "    \"aug_prob\": 0.5,\n",
        "    \"lr\": 0.1,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"amp\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "Dj5afHL9BkWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50 CIFAR-100"
      ],
      "metadata": {
        "id": "119xYLjkFQhC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "053-lvam1kUF"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb3SNqcJ9nPq"
      },
      "outputs": [],
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": None,\n",
        "    \"architecture\": \"resnet50\",\n",
        "    \"epochs\": 300,\n",
        "}\n",
        "\n",
        "resnet50_cifar_baseline = run_experiment(\"resnet50_cifar_baseline\", CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft9_i0T49uRm"
      },
      "source": [
        "## CutMix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A--hexVxSG6Z"
      },
      "outputs": [],
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": \"cutmix\",\n",
        "    \"architecture\": \"resnet50\",\n",
        "    \"epochs\": 300,\n",
        "}\n",
        "\n",
        "resnet50_cifar_cutmix = run_experiment(\"resnet50_cifar_cutmix\", CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cutout"
      ],
      "metadata": {
        "id": "RbCKSpVytbBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": \"cutout\",\n",
        "    \"architecture\": \"resnet50\",\n",
        "    \"epochs\": 300,\n",
        "}\n",
        "\n",
        "resnet50_cifar_cutout = run_experiment(\"resnet50_cifar_cutout\", CONFIG)"
      ],
      "metadata": {
        "id": "ei8u0zeetdra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixup"
      ],
      "metadata": {
        "id": "CPCSziqMthZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": \"mixup\",\n",
        "    \"architecture\": \"resnet50\",\n",
        "    \"epochs\": 300,\n",
        "}\n",
        "\n",
        "resnet50_cifar_mixup = run_experiment(\"resnet50_cifar_mixup\", CONFIG)"
      ],
      "metadata": {
        "id": "93mqqrh_tjnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-18 CIFAR-100"
      ],
      "metadata": {
        "id": "FZjvvYroFpBh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddt8Gj7k8Plf"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj-drTn9K-_y"
      },
      "outputs": [],
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": None,\n",
        "    \"architecture\": \"resnet18\",\n",
        "    \"epochs\": 200,\n",
        "}\n",
        "\n",
        "resnet18_cifar_baseline = run_experiment(\"resnet18_cifar_baseline\", CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CutMix"
      ],
      "metadata": {
        "id": "rmH6Wp23Ca1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": \"cutmix\",\n",
        "    \"architecture\": \"resnet18\",\n",
        "    \"epochs\": 200,\n",
        "}\n",
        "\n",
        "resnet18_cifar_cutmix = run_experiment(\"resnet18_cifar_cutmix\", CONFIG)"
      ],
      "metadata": {
        "id": "KKyk_L0SCcWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Y80pGNkehE"
      },
      "source": [
        "## Cutout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yau4D3sEXXvU"
      },
      "outputs": [],
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": \"cutout\",\n",
        "    \"architecture\": \"resnet18\",\n",
        "    \"epochs\": 200,\n",
        "}\n",
        "\n",
        "resnet18_cifar_cutout = run_experiment(\"resnet18_cifar_cutout\", CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oGl1HoIkiFq"
      },
      "source": [
        "## MixUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu10TI8NOUd5"
      },
      "outputs": [],
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"cifar100\",\n",
        "    \"aug_mode\": \"mixup\",\n",
        "    \"architecture\": \"resnet18\",\n",
        "    \"epochs\": 200,\n",
        "}\n",
        "\n",
        "resnet18_cifar_mixup = run_experiment(\"resnet18_cifar_mixup\", CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50 Tiny ImageNet"
      ],
      "metadata": {
        "id": "hIaE-DLw4z9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "sab-6UTf5BDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"tinyimagenet\",\n",
        "    \"aug_mode\": None,\n",
        "    \"architecture\": \"resnet50\",\n",
        "    \"epochs\": 300,\n",
        "}\n",
        "\n",
        "resnet50_tiny_baseline = run_experiment(\"resnet50_tiny_baseline\", CONFIG)"
      ],
      "metadata": {
        "id": "oW3r59Ti43-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CutMix"
      ],
      "metadata": {
        "id": "1WnYbOdHBgEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = GLOBAL_CONFIG | {\n",
        "    \"dataset\": \"tinyimagenet\",\n",
        "    \"aug_mode\": \"cutmix\",\n",
        "    \"architecture\": \"resnet50\",\n",
        "    \"epochs\": 300,\n",
        "}\n",
        "\n",
        "resnet50_tiny_cutmix = run_experiment(\"resnet50_tiny_cutmix\", CONFIG)"
      ],
      "metadata": {
        "id": "oNrgqyB_uDtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Figures"
      ],
      "metadata": {
        "id": "gH90e69ybkUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training History"
      ],
      "metadata": {
        "id": "Lgk24c783Q7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Dict, List, Sequence, Tuple, Optional, Callable\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Loading utilities\n",
        "# -----------------------------\n",
        "\n",
        "def load_runs(run_specs: Sequence[Tuple[str, str]], base_dir: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    run_specs: list of (relative_path, label)\n",
        "      relative_path is relative to base_dir (no suffix assumptions).\n",
        "      label is what will appear in the legend.\n",
        "    Returns a list of dicts with fields: label, history.\n",
        "    \"\"\"\n",
        "    runs = []\n",
        "    for rel_path, label in run_specs:\n",
        "        ckpt_path = base_dir / rel_path\n",
        "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "        history = ckpt.get(\"history\", ckpt)  # supports either layout\n",
        "        runs.append({\"label\": label, \"history\": history})\n",
        "    return runs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Plotting utilities\n",
        "# -----------------------------\n",
        "\n",
        "def plot_metric(\n",
        "    runs: Sequence[Dict],\n",
        "    metric_key: str,\n",
        "    *,\n",
        "    title: str,\n",
        "    ylabel: str,\n",
        "    xlabel: str = \"Epoch\",\n",
        "    out_dir: Path = Path(\"figures\"),\n",
        "    out_name: Optional[str] = None,\n",
        "    dpi: int = 300,\n",
        "    x_lim: Optional[Tuple[float, float]] = None,\n",
        "    y_lim: Optional[Tuple[float, float]] = None,\n",
        "    show: bool = True,\n",
        "    transform: Optional[Callable[[List[float]], List[float]]] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    metric_key should be something like:\n",
        "      'train_loss', 'val_loss', 'val_top1', 'val_top5', 'lr', etc.\n",
        "    transform: optional function applied to the series before plotting\n",
        "               (e.g., accuracy -> error).\n",
        "    Saves both PNG and PDF for LaTeX.\n",
        "    \"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_name = out_name or metric_key\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    for r in runs:\n",
        "        series = r[\"history\"][metric_key]\n",
        "        if transform is not None:\n",
        "            series = transform(series)\n",
        "        plt.plot(series, label=r[\"label\"])\n",
        "\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(xlabel, fontsize=14)\n",
        "    plt.ylabel(ylabel, fontsize=14)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    if x_lim is not None:\n",
        "        plt.xlim(*x_lim)\n",
        "    if y_lim is not None:\n",
        "        plt.ylim(*y_lim)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    png_path = out_dir / f\"{out_name}.png\"\n",
        "    pdf_path = out_dir / f\"{out_name}.pdf\"\n",
        "    plt.savefig(png_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.savefig(pdf_path, bbox_inches=\"tight\")  # vector, great for LaTeX\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "acc_to_err = lambda s: [100.0 - v for v in s]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "resnet50_cifar_specs = [\n",
        "    (r\"resnet50_cifar_baseline.pt\", \"Baseline\"),\n",
        "    # (r\"resnet50_cifar_cutout.pt\",   \"Cutout\"),\n",
        "    # (r\"resnet50_cifar_mixup.pt\",    \"MixUp\"),\n",
        "    (r\"resnet50_cifar_cutmix.pt\",   \"CutMix\"),\n",
        "]\n",
        "resnet18_cifar_specs = [\n",
        "    (r\"resnet18_cifar_baseline.pt\", \"Baseline\"),\n",
        "    (r\"resnet18_cifar_cutout.pt\",   \"Cutout\"),\n",
        "    (r\"resnet18_cifar_mixup.pt\",    \"MixUp\"),\n",
        "    (r\"resnet18_cifar_cutmix.pt\",   \"CutMix\"),\n",
        "]\n",
        "resnet50_tiny_specs = [\n",
        "    (r\"resnet50_tiny_baseline.pt\", \"Baseline\"),\n",
        "    (r\"resnet50_tiny_cutmix.pt\",   \"CutMix\"),\n",
        "]\n",
        "\n",
        "runs_50_cifar = load_runs(resnet50_cifar_specs, RESULTS_DIR)\n",
        "runs_18_cifar = load_runs(resnet18_cifar_specs, RESULTS_DIR)\n",
        "runs_50_tiny = load_runs(resnet50_tiny_specs, RESULTS_DIR)\n",
        "\n",
        "\n",
        "plot_metric(\n",
        "    runs_50_cifar, \"val_top1\",\n",
        "    title=f\"ResNet-50 CIFAR-100: Validation Top-1 Error\",\n",
        "    ylabel=\"Top-1 Error (%)\",\n",
        "    out_dir=Path(\"figures/cifar100\"),\n",
        "    out_name=f\"ResNet-50 CIFAR-100_val_top1_err\",\n",
        "    x_lim=(0, 300),\n",
        "    y_lim=(15, 100),\n",
        "    dpi=400,\n",
        "    show=True,\n",
        "    transform=acc_to_err,\n",
        ")\n",
        "\n",
        "plot_metric(\n",
        "    runs_18_cifar, \"val_top1\",\n",
        "    title=f\"ResNet-18 CIFAR-100: Validation Top-1 Error\",\n",
        "    ylabel=\"Top-1 Error (%)\",\n",
        "    out_dir=Path(\"figures/cifar100\"),\n",
        "    out_name=f\"ResNet-18 CIFAR-100_val_top1_err\",\n",
        "    x_lim=(0, 200),\n",
        "    y_lim=(15, 100),\n",
        "    dpi=400,\n",
        "    show=True,\n",
        "    transform=acc_to_err,\n",
        ")\n",
        "\n",
        "plot_metric(\n",
        "    runs_50_tiny, \"val_top1\",\n",
        "    title=f\"ResNet-50 Tiny ImageNet: Validation Top-1 Error\",\n",
        "    ylabel=\"Top-1 Error (%)\",\n",
        "    out_dir=Path(\"figures/tinyimagenet\"),\n",
        "    out_name=f\"ResNet-50 Tiny ImageNet_val_top1_err\",\n",
        "    x_lim=(0, 300),\n",
        "    y_lim=(15, 100),\n",
        "    dpi=400,\n",
        "    show=True,\n",
        "    transform=acc_to_err,\n",
        ")\n"
      ],
      "metadata": {
        "id": "0Acnt5xba07Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation Examples"
      ],
      "metadata": {
        "id": "TgIT35UtnnWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colab setup ---\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "\n",
        "def load_image(path, size=None):\n",
        "    \"\"\"Load RGB image. Optionally resize to (H,W).\"\"\"\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if size is not None:\n",
        "        img = img.resize((size[1], size[0]), Image.BILINEAR)  # PIL takes (W,H)\n",
        "    return img\n",
        "\n",
        "def pil_to_tensor(img):\n",
        "    \"\"\"PIL -> float tensor in [0,1], shape (C,H,W).\"\"\"\n",
        "    return TF.to_tensor(img)\n",
        "\n",
        "def tensor_to_pil(x):\n",
        "    \"\"\"float tensor (C,H,W) in [0,1] -> PIL.\"\"\"\n",
        "    x = x.clamp(0, 1)\n",
        "    return TF.to_pil_image(x)\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Augmentations (deterministic / controllable)\n",
        "# -------------------------\n",
        "\n",
        "def apply_mixup(x1, x2, lam=0.5):\n",
        "    \"\"\"\n",
        "    x1, x2: float tensors (C,H,W) in [0,1]\n",
        "    lam: mixing coefficient for primary image\n",
        "    \"\"\"\n",
        "    return lam * x1 + (1 - lam) * x2\n",
        "\n",
        "def apply_cutout(x, erase_frac=0.25, fill=0.0, center=None, seed=None):\n",
        "    \"\"\"\n",
        "    x: float tensor (C,H,W)\n",
        "    erase_frac: fraction of total area to erase (e.g., 0.25 = 25%)\n",
        "    fill: fill value in [0,1]\n",
        "    center: (cy, cx) in pixels. If None, random location.\n",
        "    seed: for reproducible random location.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        set_seed(seed)\n",
        "\n",
        "    C, H, W = x.shape\n",
        "    erase_area = erase_frac * H * W\n",
        "    side = int(round(math.sqrt(erase_area)))  # square patch\n",
        "\n",
        "    if center is None:\n",
        "        cy = random.randint(0, H-1)\n",
        "        cx = random.randint(0, W-1)\n",
        "    else:\n",
        "        cy, cx = center\n",
        "\n",
        "    y1 = max(0, cy - side // 2)\n",
        "    y2 = min(H, y1 + side)\n",
        "    x1 = max(0, cx - side // 2)\n",
        "    x2 = min(W, x1 + side)\n",
        "\n",
        "    out = x.clone()\n",
        "    out[:, y1:y2, x1:x2] = fill\n",
        "    return out\n",
        "\n",
        "def rand_bbox(H, W, lam, seed=None):\n",
        "    \"\"\"Standard CutMix bbox sampling given lambda.\"\"\"\n",
        "    if seed is not None:\n",
        "        set_seed(seed)\n",
        "\n",
        "    cut_ratio = math.sqrt(1.0 - lam)\n",
        "    cut_w = int(W * cut_ratio)\n",
        "    cut_h = int(H * cut_ratio)\n",
        "\n",
        "    cx = random.randint(0, W-1)\n",
        "    cy = random.randint(0, H-1)\n",
        "\n",
        "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "def apply_cutmix(x1, x2, lam=0.5, bbox=None, seed=None):\n",
        "    \"\"\"\n",
        "    x1 primary, x2 secondary. Pastes patch from x2 into x1.\n",
        "    lam: area fraction kept from primary (approx; exact depends on bbox)\n",
        "    bbox: (x1,y1,x2,y2). If None, sample from lam.\n",
        "    seed: reproducible random bbox sampling.\n",
        "    Returns: cutmixed tensor, effective_lambda\n",
        "    \"\"\"\n",
        "    C, H, W = x1.shape\n",
        "    if bbox is None:\n",
        "        x1b, y1b, x2b, y2b = rand_bbox(H, W, lam, seed=seed)\n",
        "    else:\n",
        "        x1b, y1b, x2b, y2b = bbox\n",
        "\n",
        "    out = x1.clone()\n",
        "    out[:, y1b:y2b, x1b:x2b] = x2[:, y1b:y2b, x1b:x2b]\n",
        "\n",
        "    # effective lambda based on actual area\n",
        "    patch_area = (x2b - x1b) * (y2b - y1b)\n",
        "    eff_lam = 1.0 - patch_area / (H * W)\n",
        "    return out, eff_lam\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Figure creation\n",
        "# -------------------------\n",
        "\n",
        "def make_augmentation_row(\n",
        "    primary_path,\n",
        "    secondary_path,\n",
        "    *,\n",
        "    primary_label=\"Primary\",\n",
        "    secondary_label=\"Secondary\",\n",
        "    out_path=\"augmentation_row.png\",\n",
        "    size=(64, 64),  # (H,W); set None to keep original\n",
        "    mixup_lam=0.5,\n",
        "    cutout_frac=0.25,\n",
        "    cutout_fill=0.0,\n",
        "    cutout_center=None,  # e.g., (H//2, W//2)\n",
        "    cutmix_lam=0.5,\n",
        "    cutmix_bbox=None,    # e.g., (x1,y1,x2,y2)\n",
        "    seed=0,\n",
        "    dpi=400,\n",
        "):\n",
        "    \"\"\"\n",
        "    Produces 1x4 row: Baseline | MixUp | Cutout | CutMix.\n",
        "    Labels technique above, class/mix label below.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Load + match sizes\n",
        "    img1 = load_image(primary_path, size=size)\n",
        "    img2 = load_image(secondary_path, size=size)\n",
        "\n",
        "    x1 = pil_to_tensor(img1)\n",
        "    x2 = pil_to_tensor(img2)\n",
        "\n",
        "    # Baseline\n",
        "    baseline_img = tensor_to_pil(x1)\n",
        "    baseline_text = primary_label + \" 1.0\"\n",
        "\n",
        "    # MixUp\n",
        "    mixup_x = apply_mixup(x1, x2, lam=mixup_lam)\n",
        "    mixup_img = tensor_to_pil(mixup_x)\n",
        "    mixup_text = f\"{primary_label} {mixup_lam:.2f} \\n {secondary_label} {1-mixup_lam:.2f}\"\n",
        "\n",
        "    # Cutout\n",
        "    cutout_x = apply_cutout(\n",
        "        x1, erase_frac=cutout_frac, fill=cutout_fill,\n",
        "        center=cutout_center, seed=seed\n",
        "    )\n",
        "    cutout_img = tensor_to_pil(cutout_x)\n",
        "    cutout_text = primary_label + \" 1.0\"\n",
        "\n",
        "    # CutMix\n",
        "    cutmix_x, eff_lam = apply_cutmix(\n",
        "        x1, x2, lam=cutmix_lam,\n",
        "        bbox=cutmix_bbox, seed=seed\n",
        "    )\n",
        "    cutmix_img = tensor_to_pil(cutmix_x)\n",
        "    cutmix_text = f\"{primary_label} {eff_lam:.2f} \\n {secondary_label} {1-eff_lam:.2f}\"\n",
        "\n",
        "    panels = [\n",
        "        (\"Baseline\", baseline_img, baseline_text),\n",
        "        (\"Mixup\",    mixup_img,    mixup_text),\n",
        "        (\"Cutout\",   cutout_img,   cutout_text),\n",
        "        (\"CutMix\",   cutmix_img,   cutmix_text),\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
        "    for ax, (title, im, lbl) in zip(axes, panels):\n",
        "        ax.imshow(im)\n",
        "        ax.set_title(title, fontsize=20, pad=10)   # technique ABOVE\n",
        "        ax.set_xlabel(lbl, fontsize=20, labelpad=8)  # label BELOW\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved to {out_path}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Example usage (edit these)\n",
        "# -------------------------\n",
        "\n",
        "ensure_tinyimagenet()\n",
        "# Put your two image paths here. In Colab you can upload, then use /content/filename.jpg\n",
        "primary_path = \"/content/data/tiny-imagenet-200/train/n03100240/images/n03100240_103.JPEG\"\n",
        "secondary_path = \"/content/data/tiny-imagenet-200/train/n01443537/images/n01443537_10.JPEG\"\n",
        "\n",
        "make_augmentation_row(\n",
        "    primary_path,\n",
        "    secondary_path,\n",
        "    primary_label=\"Convertible\",\n",
        "    secondary_label=\"Goldfish\",\n",
        "    size=(64, 64),          # match Tiny ImageNet; use (32,32) for CIFAR\n",
        "    mixup_lam=0.5,\n",
        "    cutout_frac=0.25,\n",
        "    cutout_fill=0.0,\n",
        "    cutout_center=(16, 40),     # or (32,32) for deterministic center\n",
        "    cutmix_lam=0.75,\n",
        "    cutmix_bbox=(24, 0, 56, 32),       # or set fixed box like (10,10,40,40)\n",
        "    seed=3,\n",
        "    out_path=\"aug_row_tinyimagenet.pdf\",\n",
        "    dpi=500,\n",
        ")\n"
      ],
      "metadata": {
        "id": "rl3dyT9_WHHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrh564C0wquhkMnbq5yeTC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}